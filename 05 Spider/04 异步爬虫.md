高性能异步爬虫：
目的： 在爬虫中使用异步实现高性能的数据爬取操作

# 单线程串行方式
```python
import requests
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                         'Chrome/84.0.4147.89 Safari/537.36'}
urls={
    'http://www.baidu.com',
    'http://www.163.com',
    'http://www.sina.com',
}
def get_content(url):
    # get 方法是一个阻塞的方法，一次一个
    response=requests.get(url=url,headers=headers)
    if response.status_code==200:
        return response.text
def parse_content(content):
    print('响应数据的长度为:',len(content))
for url in urls:
    content=get_content(url)
    parse_content(content)
```

# 异步爬虫的方式
* 多线程，多进程
    * 好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作可以异步执行
    * 弊端： 无法无限制的开启多线程或者多进程
* 线程池，进程池：
    * 好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销
    * 弊端：池中线程或进程的数量是有上限的

# 线程池
### 单线程
```python
import time
def get_page(str):
    time.sleep(2)
name_list=['a','b','c','d']
start_time=time.time()
for name in name_list:
    get_page(name)
end_time=time.time()
print(end_time-start_time) #8.00108814239502
```
### 线程池
```python
import time
from multiprocessing.dummy import Pool
start_time=time.time()
def get_page(str):
    time.sleep(2)
name_list=['a','b','c','d']
pool=Pool(4)
pool.map(get_page,name_list) #返回值是一个列表
end_time=time.time()
print(end_time-start_time)#2.0140514373779297
```

# 爬取梨视频的视频数据
```python
import requests
from lxml import etree
import re
from multiprocessing.dummy import Pool
# 需求，爬取视频数据
def getUrls():
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                             'Chrome/84.0.4147.89 Safari/537.36'}
    url='http://www.pearvideo.com/category_5'
    page_text=requests.get(url=url,headers=headers).text
    tree=etree.HTML(page_text)
    li_list=tree.xpath('//ul[@id="listvideoListUl"]/li')
    urls=[]
    for li in li_list:
        detail_url="http://www.pearvideo.com/"+li.xpath('./div/a/@href')[0]
        name=li.xpath('./div/a/div[2]/text()')[0]+'.mp4'
        detail_url_page=requests.get(url=detail_url,headers=headers).text
        ex='srcUrl="(.*?)",vdoUrl'
        video_url=re.findall(ex,detail_url_page)[0]
        dic={'name':name,
              'url':video_url}
        urls.append(dic)
    return urls

def get_video_data(dic):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                             'Chrome/84.0.4147.89 Safari/537.36'}
    url=dic['url']
    print(dic['name'], ' start')
    data=requests.get(url=url,headers=headers).content
    with open(dic['name'],'wb') as fp:
        fp.write(data)
        print(dic['name'],' Done')

def download(urls):
    pool = Pool(4)
    pool.map(get_video_data, urls)
    pool.close()#关闭进程池，不再接受新的进程
    pool.join()#主进程阻塞后，让子进程继续运行完成，子进程运行完后，再把主进程全部关掉。

if __name__=='__main__':
    urls=getUrls()
    download(urls)
```
# 单线程+异步协程（推荐）
* event_loop:事件循环，相当于一个无限循环，我们可以吧一些函数注册到这个事件循环上，
当满足某些条件的时候，函数就会被循环执行
* coroutine:协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以
使用async关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象
* task：任务，他是对协程对象的进一步封装，包含了任务的各个状态
* future： 代表将来执行或还没有执行的任务，实际上和task没有本质区别
* async： 定义一个协程
* await： 用来挂起阻塞方法的执行

```python
import asyncio
async def request(url):
    print('start')
    print('end')
    return url
c=request('www.baidu.com')
```
```python
# 创建一个事件循环对象
loop=asyncio.get_event_loop()
# 将协程对象注册到loop中，然后启动loop
loop.run_until_complete(c)
```
task的使用
```python
loop=asyncio.get_event_loop()
task=loop.create_task(c)
print(task)#<Task pending coro=<request() running at D:/study/Spider/test.py:2>>
loop.run_until_complete(task)
print(task)#<Task finished coro=<request() done, defined at D:/study/Spider/test.py:2> result=None>
```
future的使用
```python
loop=asyncio.get_event_loop()
task=asyncio.ensure_future(c)
print(task)#<Task pending coro=<request() running at D:/study/Spider/test.py:2>>
loop.run_until_complete(task)
print(task)#<Task finished coro=<request() done, defined at D:/study/Spider/test.py:2> result=None>
```
绑定回调
```python
def callback_func(task):
    #result返回的就是任务对象中封装的协程对象对应函数的返回值
    print(task.result())
loop=asyncio.get_event_loop()
task=asyncio.ensure_future(c)
task.add_done_callback(callback_func)
loop.run_until_complete(task)
```

# 多任务异步协程
```python
import asyncio
import time

async def request(url):
    print("start")
    # 在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步
    #time.sleep(2) # 6
    #当在asyncio中遇到堵塞操作必须进行手动挂起
    await asyncio.sleep(2) # 2.002054452896118
    print("done!")

start=time.time()
urls=['www.baidu.com','www.sogou.com','www.douban.com']

#任务列表
tasks=[]
for url in urls:
    c=request(url)
    task=asyncio.ensure_future(c)
    tasks.append(task)

loop=asyncio.get_event_loop()
#需要将任务列表封装到wait中
loop.run_until_complete(asyncio.wait(tasks))
print(time.time()-start)
```

# 多任务异步协程
```python
import requests
import asyncio
import time
import aiohttp

start=time.time()
urls=['http://127.0.0.1:5000/1',
    'http://127.0.0.1:5000/2',
    'http://127.0.0.1:5000/3']

async def get_page(url):
    # requests.get是基于同步的，必须使用基于异步的网络请求模块进行指定的url请求发送
    # aiohttp:基于异步的网络
    # response = requests.get(url=url)
    async with aiohttp.ClientSession() as session:
        # headers,params/data,proxy=''
        async with await session.get(url) as response:
            #text()返回字符串形式的响应数据
            #read()二级制
            #json()对象
            #注意：获取响应数据操作之前一定要使用await进行手动挂起
            page_text=await response.text()
            print(page_text)
tasks=[]
for url in urls:
    c=get_page(url)
    task=asyncio.ensure_future(c)
    tasks.append(task)

loop=asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
end=time.time()
print(end-start)
# requests.get 3s,还是串行的
#aiohttp 1.0083019733428955，aiohttp 需要pip安装
```
* 搭建测试服务器
```python
from flask import Flask
import time

app=Flask(__name__)

@app.route('/1')
def index1():
    time.sleep(1)
    return 'hello 1'

@app.route('/2')
def index2():
    time.sleep(1)
    return 'hello 2'

@app.route('/3')
def index3():
    time.sleep(1)
    return 'hello 3'

if __name__=='__main__':
    app.run(threaded=True)
```