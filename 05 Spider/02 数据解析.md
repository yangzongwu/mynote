# 数据解析
聚焦爬虫：爬取页面中指定的页面内容。  
编码流程：  
* 指定url
* 发起请求
* 获取响应数据
* 数据解析
* 持久化存储
### 数据解析分类
* 正则
* bs4
* xpath(通用性比较强)
### 数据解析原理概述
* 解析的局部文本内容都会在标签之前或者标签对应的属性中进行存储
* 进行指定标签的定位
* 标签或者标签对应的属性中存储的数据进行提取(解析)

# 正则
### 如何获取图片
```python
import requests
def getPic():
    url='https://pic.qiushibaike.com/article/image/L4DQGEAAH1QT50K8.jpg'
    # text(字符串),content(二进制)，json(json文档）
    img_data=requests.get(url=url).content
```
### 案列 爬取糗事百科所有图片
```python
import requests
import re
import os
def getPic():
    if not os.path.exists('./qiutu'):
        os.mkdir('./qiutu')
    url='https://www.qiushibaike.com/article/123395129'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}
    page_text=requests.get(url=url,headers=headers).text
    ex='<img src="//(pic.qiushibaike.com/article/image/.*?.jpg)" alt=.*?>'
    img_src_list=re.findall(ex,page_text,re.S)
    for src in img_src_list:
        src= 'https://'+src
        img_data=requests.get(url=src,headers=headers).content
        img_name=src.split('/')[-1]
        imgPath='./qiutu/'+img_name
        with open(imgPath,'wb') as fp:
            fp.write(img_data)
            print(img_name,'done!')
```
# bs4
### 概述
python独有的  
* 数据解析原理概述
    * 标签定位
    * 标签或者标签对应的属性中存储的数据进行提取(解析)
* bs4数据解析的原理
    * 实例化一个BeatifulSoup对象，并且将页面源码数据加载到该对象中
    * 通过调用BeatifulSoup对象中相关的属性或者方法进行标签定位和数据提取
* 环境安装：
    * pip install bs4
    * pip install lxml
### 如何实例化：
   * from bs4 import BeautifulSoup
   * 对象实例化
        * 本地HTML文档加载
            ```
            fp=open('./test.html','r',encoding='utf-8')
            soup=BeautifulSoup(fp,'lxml')
            ```
        * 互联网页面加载
            ```
            page_text=response.text
            soup=BeautifulSoup(page_text,'lxml')
            ```
### 提供的属性和方法
```
soup.tagName #打印第一次出现的tagName标签,soup.div,soup.a 
soup.find('div') # 等同于soup.tagName
soup.find('div'，class_/id/attr='2') #属性定位，class是关键字所以带_
soup.find_all('tagName') #返回所有的，是一个列表
```
```
soup.select('某种选择器(id，class，标签。。。)') # 返回一个列表，如.3
soup.select（'.3>ul>li>a')[0]#层级选择器
soup.select（'.3>ul a')[0]#空格表示多个层级，>一个层级
```
### 获取数据
* 获取标签之间的文本数据
        ```
        soup.a.text/string/get_text()
        ```  
    * text，get_text()：可以获得该标签下所有的本文内容
    * string：可以获得该标签下直系的本文内容
* 获取标签中属性值  
        ```
        soup.a['href']
        ```
### 案例 获取三国演义标题与文本
```python
from bs4 import BeautifulSoup
import requests

# 需求，爬取三国演义小说所有的章节标题和其对应的内容
def bs4():
    # 对首页页面数据爬取
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}
    url = 'http://www.shicimingju.com/book/sanguoyanyi.html'
    page_text = requests.get(url=url, headers=headers).text
    soup = BeautifulSoup(page_text, 'lxml')
    li_list = soup.select('.book-mulu>ul>li')
    fp=open('./sanguo.txt','w',encoding='utf-8')
    for li in li_list:
        title = li.a.string
        detail_url = 'http://www.shicimingju.com' + li.a['href']
        # 解析内容
        detail_page_text = requests.get(url=detail_url, headers=headers).text
        detail_soup = BeautifulSoup(detail_page_text, 'lxml')
        div_tag = detail_soup.find('div', class_='chapter_content')
        content = div_tag.text
        fp.write(title+':'+content+'\n')
        print(title,'Done!')

bs4()
```

# xpath
### 解析原理
* 实例化一个etree对象，且需要将被间隙的页面源码数据加载到该对象中
* 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获
### 环境的安装
```
pip install lxml
```
### 实例化
```
from lxml import etree
```
* 将本地的html文章中的源码数据加载到etree对象中
    ```
    etree.parse(filePath)
    ```
* 可以将从互联网上获取的源码数据加载到该对象中
    ```
    etree.HTML('page_tex')
    ```
* xpath('xpath表达式')

### xpath 表达式
* /:表示从根节点开始定位；表示一个层级
* //表示多个层级，可以表示从任意位置开始
* 属性定位：//div[@class='du'],tag[@attrName='attrValue']
* 索引定位，div[@class='du']/p[3]，索引从1开始
    ```
    tree=etree.parse('test.html')
    r=tree.xpath('/html/head/title')# [<title>head</title>]
    r=tree.xpath('/html/body/div') # [***,***,***]
    r=tree.xpath('/html//div') # 和上面一样[***,***,***]
    r=tree.xpath('//div') # 任意位置寻找div，[***,***,***]
    r=tree.xpath('//div[@class='du']') #[***]
    r=tree.xpath('//div[@class='du']/p[3]')#[***]
    ```
* 取值
    * /text(),标签中直系内容
    * //text(),标签中所有的内容
    ```
    tree=etree.parse('test.html')
    r=tree.xpath('//div[@class='tang']//li[5]/a/text()')[0]
    r=tree.xpath('//li[7]//text()')   
    ```
* 取属性
    * /@attrName ==> img/@src
    ```
    tree=etree.parse('test.html')
    r=tree.xpath('//div[@class='tang']/img/@src)
    ```
### 案例一
爬取58二手房信息
```python
from lxml import etree
import requests
def _58TongCheng():
    url = 'https://bj.58.com/ershoufang/'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}
    page_text=requests.get(url,headers=headers).text
    tree=etree.HTML(page_text)
    li_list=tree.xpath('//ul[@class="house-list-wrap"]/li')
    fp=open('58.txt','w',encoding='utf-8')
    for li in li_list:
        title=li.xpath('./div[2]/h2/a/text()')[0]
        fp.write(title+'\n')
_58TongCheng()
```

### 案例二
解析下载图片,注意乱码处理
```python
import os

from lxml import etree
import requests
def _58TongCheng():
    url = 'http://pic.netbian.com/4kmeinv/'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}
    response = requests.get(url, headers=headers)
    # 手动设定响应数据的编码格式
    #response.encode = 'utf-8'
    page_text = response.text
    tree=etree.HTML(page_text)
    li_list=tree.xpath('//div[@class="slist"]/ul[@class="clearfix"]/li')

    if not os.path.exists('./pic'):
        os.mkdir('./pic')
    for li in li_list:
        img_src='http://pic.netbian.com'+li.xpath('./a/img/@src')[0]
        img_name=li.xpath('./a/img/@alt')[0]+'.jpg'
        #通用处理中文乱码的解决方案
        img_name=img_name.encode('iso-8859-1').decode('gbk')
        img_data=requests.get(url=img_src,headers=headers).content
        img_path='pic/'+img_name
        with open(img_path,'wb') as fp:
            fp.write(img_data)
            print(img_name,'Done!')
_58TongCheng()
```
### 案例三
爬取全国城市的名称  
注意|符号用法  
```python
tree.xpath('//div[@class="bottom"]/ul/li | //div[@class="bottom"]/ul/div[2]/li')

```
```python
from lxml import etree
import requests
def Allcity():
    url = 'https://www.aqistudy.cn/historydata/'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}
    page_text = requests.get(url=url, headers=headers).text
    tree=etree.HTML(page_text)
    hot_li_list=tree.xpath('//div[@class="bottom"]/ul/li')
    all_city_names=[]
    for li in hot_li_list:
        hot_city_name=li.xpath('./a/text()')[0]
        all_city_names.append(hot_city_name)
    city_names_list=tree.xpath('//div[@class="bottom"]/ul/div[2]/li')
    for li in city_names_list:
        city_name=li.xpath('./a/text()')[0]
        all_city_names.append(city_name)
    print(len(all_city_names))
    a_list=tree.xpath('//div[@class="bottom"]/ul/li | //div[@class="bottom"]/ul/div[2]/li')
    all_city=[]
    for a in a_list:
        city_name=a.xpath('./a/text()')[0]
        all_city.append(city_name)
    print(len(all_city))
Allcity()
```

### 案例四
爬取免费简历模板
```python
import os

from lxml import etree
import requests
def resumes():
    url = 'http://sc.chinaz.com/jianli/free.html'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}
    response=requests.get(url=url, headers=headers)
    response.encoding = 'GBK'
    response.encoding = 'utf-8'
    page_text=response.text
    tree=etree.HTML(page_text)
    a_lists=tree.xpath('//div[@id="container"]/div')
    if not os.path.exists('./resume'):
        os.mkdir('./resume')
    for a_list in a_lists:
        herf_link=a_list.xpath('./a/@href')[0] # 简历页面 eg:http://sc.chinaz.com/jianli/200728114960.htm
        resume_name=a_list.xpath('./p/a/text()')[0]+'.rar'
        download_path='resume/'+resume_name
        resume_page_text=requests.get(url=herf_link,headers=headers).text
        resume_tree=etree.HTML(resume_page_text)
        resume_link=resume_tree.xpath('//div[@id="down"]/div[2]/ul/li[1]/a')[0]
        download_link=resume_link.xpath('./@href')[0]
        download_date = requests.get(url=download_link, headers=headers).content
        with open(download_path, 'wb') as fp:
            fp.write(download_date)
resumes()
```