* 概念
    * 监测网站数据更新情况，只爬取网站最新更新出来的数据
* 分析：
    * 指定起始url
    * 基于CrawlSpider获取其他页面链接
    * 基于Rule将其他页码链接进行请求
    * 从每一个页码对应的页面源吗中解析出每一个电影详情页的URL
    * 【新】检测电影详情页的url之前有没有请求过
        * 将爬取过的电影详情页的url存储
            * 存储到redis的set数据结构中
    * 对详情页的url发起请求，然后解析出电影的名称和简介
    * 进行持久化存储

###### --sun.py--
```python
class SunSpider(RedisCrawlSpider):
    name = 'sun'
    redis_key='pic'
    rules = (Rule(LinkExtractor(allow=r'list[0-9]*\.html'), callback='parse_item', follow=True),)
    conn=Redis(host='127.0.0.1',port=6379)
    def parse_item(self, response):
        li_list=response.xpath('//*[@id="content"]/div[2]/div[2]/ul/li')
        for li in li_list:
            name=li.xpath('./a[2]/b/text()').extract_first()
            detail_url=li.xpath('./a[1]/@href').extract_first()
            ex=self.conn.sadd('urls',detail_url)
            if ex==1:
                yield scrapy.Request(url=detail_url,callback=self.parse_detail)
            else:
                print("already exist")
    def parse_detail(self,response):
        item=SunproItem()
        item['name']='xxx'
        item['detail']='xxx'
        yield item
```
###### --items.py--
```python
class SunproItem(scrapy.Item):
    name=scrapy.Field()
    detail_url=scrapy.Field()
```
###### --pipeline.py--
```python
class SunproPipeline:
    conn=None
    def open_spider(self,spider):
        self.conn=spider.conn
    def process_item(self, item, spider):
        dic={
            'name':item['name'],
            'detail':item['detail']
        }
        self.conn.lpush('totalData',dic)
        return item
```